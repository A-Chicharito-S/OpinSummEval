{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 1\n\nexplanation: the summary is completely inaccurate and irrelevant to the reviews provided. it seems like the summary was generated by mistake and does not reflect the content of the reviews."}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}}, "case": 1}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 2}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 3}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 2/5\nthe summary does not accurately represent the reviews as it mentions food and a line to get in, which were not mentioned in any of the reviews. additionally, the language used is not fluent and the structure of the summary is unclear."}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 4}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 5, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 5}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 6}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 1\n\nthe summary is completely unrelated to the reviews and does not accurately reflect the experiences shared by the customers."}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 1\n\nthe summary is not relevant to the reviews provided and does not accurately reflect the experiences shared by the reviewers. it also includes a complaint that is not mentioned in any of the reviews."}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 1\n\nthe summary does not accurately represent the reviews provided. it only focuses on one doctor and does not mention any of the positive or negative experiences shared by the reviewers. additionally, the summary is not fluent or informative, as it is repetitive and lacks detail."}}, "case": 7}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 8}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 2\nthe summary is not relevant to the reviews provided. it talks about super fresh food, fast service, and organic products, which are not mentioned in any of the reviews. the summary also ends with a confusing statement about being charged for organic products, which is not explained or related to the reviews. the language used is not clear and the structure of the summary is poor, making it difficult to understand."}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}}, "case": 9}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 10}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 11}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 12}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 1\n\nthe summary does not accurately reflect the reviews and is not informative or fluent. it appears to be a generic statement that does not provide any specific details about the restaurant or its offerings."}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 13}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 14}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 15}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 16}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 17}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 18}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 19}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 20}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 5, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 1\nthe summary is completely unrelated to the reviews and does not provide any useful information about the dealership. it appears to be a random comment about food."}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 21}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 22}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 23}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 5, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 24}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 25}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 2\nthe summary does not accurately reflect the reviews provided. it focuses on a single item (chocolate cake) and does not mention the wide selection of macarons and eclairs that are consistently praised in the reviews. additionally, the summary is not very informative and lacks detail about the bakery's offerings and atmosphere."}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}}, "case": 26}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 5, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 5, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 2\n\nthe summary is not accurate as it mentions sky diving, which is not related to the reviews. it also lacks detail and does not provide a clear picture of the experiences shared by the reviewers. the language used is simple, but the structure is not well-organized, making it difficult to follow."}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}}, "case": 27}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 28}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 29}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 30}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 31}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 32}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 33}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 2\nthe summary does not accurately reflect the reviews provided. it focuses on one dish and does not mention any of the other positive or negative aspects of the restaurant mentioned in the reviews. additionally, the language used in the summary is not particularly informative or engaging."}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 34}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 35}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 2\nthe summary does not accurately represent the reviews provided. it only mentions one dessert (banana pudding) and does not provide any information about the bakery or its overall quality. additionally, the language used in the summary is not very clear or informative."}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}}, "case": 36}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}}, "case": 37}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 38}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}}, "case": 39}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 40}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 41}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 2\nthe summary does not accurately reflect the reviews and seems to be a generic statement about a place to hang out with friends and family. it does not mention any specific details about the food or beer selection, and does not address any of the concerns or praises mentioned in the reviews. the language is also not very informative or fluent."}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 2\nthe summary does not accurately reflect the reviews and seems to be a generic statement about a restaurant. it also mentions a burger, which was not mentioned in any of the reviews. the language is not fluent and the structure is unclear."}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 42}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 43}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 1\n\nthe summary is not relevant to the reviews and does not accurately reflect the experiences shared by the customers. it appears to be a generic description of the restaurant and does not provide any useful information."}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 2\nthe summary does not accurately reflect the reviews provided and is not informative or fluent. it also does not address any of the specific feedback provided in the reviews."}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 44}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 1\n\nthe summary is not a fair representation of the reviews. it is not informative and does not accurately reflect the experiences shared by the reviewers. the language used is unclear and the structure of the summary is confusing."}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 45}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 46}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 47}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 2\nthe summary does not accurately reflect the reviews provided. it seems to be a summary of a different restaurant altogether. the language used is also not very clear and the structure of the summary is confusing."}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 2\nthe summary is not relevant to the reviews provided. it does not accurately reflect the experiences shared by the customers and does not provide any useful information about the cafe/bakery. the language used is also not fluent and the structure of the summary is poor."}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 48}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 49}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 50}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 51}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 52}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 53}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 54}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 2\nthe summary is not relevant to the reviews provided and does not accurately reflect the experiences shared by the customers. the language used is also not fluent and lacks coherence."}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 55}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 56}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 57}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 5, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}}, "case": 58}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}}, "case": 59}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 60}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 61}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 62}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 5, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 63}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}}, "case": 64}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 65}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 66}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 1/5\nthe summary is completely unrelated to the reviews and does not accurately reflect the experiences shared by the customers. it is not fluent or informative and does not demonstrate an understanding of the content of the reviews."}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 1\n\nexplanation: the summary does not accurately represent the reviews and is not informative. it also contains incorrect information, as none of the reviews mention being in and out in less than 10 minutes."}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 67}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 2/5\nthe summary is not fluent and informative. it does not accurately reflect the reviews and contains irrelevant information. the language used is unclear and the structure of the summary is poor."}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 68}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 2/5\nthe summary is not fluent and informative. it is a combination of different reviews and does not provide a clear and concise overview of the restaurant. the language used is confusing and the structure of the summary is disorganized."}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 1 (very low)\nexplanation: the summary does not reflect any of the reviews and is not informative or coherent. it appears to be a random collection of phrases that do not make sense together."}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 69}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 70}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 1\n\nthe summary is completely unrelated to the reviews and seems to be describing a different place altogether. it is also poorly written and lacks coherence."}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 2\nthe summary does not accurately reflect the reviews and seems to be a generic description of a restaurant rather than a specific review of birdsong brewery. the language is not fluent and the structure is unclear, making it difficult to understand the intended message."}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 71}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 72}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}}, "case": 73}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}}, "case": 74}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 75}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 76}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 1\n\nexplanation: the summary does not accurately reflect the reviews provided. it is overly positive and does not address any of the negative experiences mentioned in the reviews. additionally, the language used in the summary is not fluent and lacks detail."}}, "case": 77}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 78}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 79}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 80}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 81}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 82}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 83}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 2\nthe summary is not relevant to the reviews provided. it does not accurately reflect the experiences shared by the reviewers."}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 84}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 85}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 86}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 87}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 88}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}}, "case": 89}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}}, "case": 90}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 91}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 1\n\nexplanation: the summary does not make sense and does not provide any useful information about the reviews. it appears to be a random sentence that does not relate to the reviews at all."}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 92}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 2/5\nthe summary is not accurate and does not reflect the reviews. it mentions breakfast, which is not mentioned in any of the reviews. it also does not mention the specific sandwiches that were recommended by the reviewers. the language used is not fluent and the structure of the summary is confusing."}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 93}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 1\n\nthe summary is completely unrelated to the reviews and does not provide any useful information about the facility. it appears to be a random collection of phrases and sentences that do not make sense together."}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 94}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 1\n\nthe summary is completely unrelated to the reviews and does not provide any useful information about the business."}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}}, "case": 95}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "rating: 2\nthe summary is not informative and lacks clarity. it is too general and does not provide any specific details about the shop or the artists. the language used is also not fluent and the structure of the summary is poor."}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}}, "case": 96}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 97}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 98}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 99}
{"model_output": {"bart": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "bertcent": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "coop": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "copycat": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "denoisesum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "gpt3": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "lexrank": {"model_summ": null, "chatgpt_CoT_Readability": 4, "model_scoring_flag": "scored"}, "meansum": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "opiniondigest": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "opinosis": {"model_summ": null, "chatgpt_CoT_Readability": 1, "model_scoring_flag": "scored"}, "pegasus": {"model_summ": null, "chatgpt_CoT_Readability": 3, "model_scoring_flag": "scored"}, "plansum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "recursum": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}, "t5": {"model_summ": null, "chatgpt_CoT_Readability": 2, "model_scoring_flag": "scored"}}, "case": 100}
